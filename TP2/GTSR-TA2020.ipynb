{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tecnologias e Aplicações\n",
    "## Trabalho prático 2, Março 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### German Trafic Sign Recognition using CNN\n",
    "\n",
    "Using the GTSR dataset for recognition of traffic signs, we created a convolutional neural network that recognizes traffic signs.\n",
    "\n",
    "To improve the model the following elements were changed:\n",
    "- batch size\n",
    "- image size\n",
    "- the number of layers (depth of the network):\n",
    "    - Padding, and its size\n",
    "    - number and size of filters for the convolution layers\n",
    "    - How many dropouts, and its percentage\n",
    "    - How many dense layers\n",
    "- number of epochs\n",
    "\n",
    "These elements are discussed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from PIL import Image\n",
    "import IPython.display as display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"gtsrb/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch size\n",
    "\n",
    "Batch size is an important parameter when training a network. It can influence speed and generalization, not necessarily in the same direction. There is no golden rule for the batch size but 32 is a commom number to start with.\n",
    "\n",
    "### Image size\n",
    "\n",
    "Bigger images means more computation operations per layer as well as more memory requirements, but better filters. So we choose 64 x 64 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare to load images\n",
    "\n",
    "Getting all the class names in a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path(dataset_path + \"train_images/\")\n",
    "trainClassNames = np.array(os.listdir(data_dir))\n",
    "\n",
    "data_dir = pathlib.Path(dataset_path + \"val_images/\")\n",
    "valClassNames = np.array(os.listdir(data_dir))\n",
    "\n",
    "print(\"training\", trainClassNames)\n",
    "print(\"validation\", valClassNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions \n",
    "\n",
    "#### get_label\n",
    "Gets the label of an image, through its path. This function returns an array of Booleans where all elements are False except one, and the position of that True represents a class.\n",
    "\n",
    "#### decode_img\n",
    "\n",
    "Process the image for better performance and usage, by converting to uint8, making it binary and resizing.\n",
    "\n",
    "#### get_bytes_and_label\n",
    "\n",
    "Receives a path for an image and returns the image and its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(file_path):\n",
    "  # convert the path to a list of path components\n",
    "  parts = tf.strings.split(file_path, os.path.sep)\n",
    "  # The second to last is the class-directory\n",
    "  return parts[-2] == classNames\n",
    "\n",
    "def decode_img(img):\n",
    "  # convert the compressed string to a 3D uint8 tensor\n",
    "  img = tf.image.decode_png(img, channels=3)\n",
    "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "  # resize the image to the desired size.\n",
    "  return tf.image.resize(img, [IMAGE_SIZE,IMAGE_SIZE])\n",
    "\n",
    "def get_bytes_and_label(file_path):\n",
    "  label = get_label(file_path)\n",
    "  # load the raw data from the file as a string\n",
    "  img = tf.io.read_file(file_path)\n",
    "  img = decode_img(img)\n",
    "  return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading images\n",
    "\n",
    "The images are loaded to a Dataset from Keras and shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "listset = tf.data.Dataset.list_files(dataset_path + \"train_images/*/*.png\")\n",
    "train_dataset = listset.map(get_bytes_and_label, num_parallel_calls = AUTOTUNE)\n",
    "\n",
    "listset = tf.data.Dataset.list_files(dataset_path + \"val_images/*/*.png\")\n",
    "val_dataset = listset.map(get_bytes_and_label, num_parallel_calls = AUTOTUNE)\n",
    "\n",
    "listset = tf.data.Dataset.list_files(dataset_path + \"test_images/*/*.png\")\n",
    "test_dataset = listset.map(get_bytes_and_label, num_parallel_calls = AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train set and validation set\n",
    "\n",
    "Due to the way the GTSRB is built (using video sequences) the set needs to be partitioned manually as to keep the sequences together in either set. This partition must guarantee that both sets contain an even amount of all classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information about image shape and size of training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, label in train_dataset.take(1):\n",
    "  print(\"Image shape: \", image.numpy().shape)\n",
    "  \n",
    "dataset_length = [i for i,_ in enumerate(train_dataset)][-1] + 1\n",
    "print(\"Total images in dataset: \",dataset_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#https://www.tensorflow.org/api_docs/python/tf/image\n",
    "#https://www.tensorflow.org/addons/api_docs/python/tfa/image\n",
    "\n",
    "#pip install tensorflow-addons\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "def process_image(image, label):\n",
    "    image = tf.image.resize(image, (64,64))\n",
    "    \n",
    "    # Devolde um numero aleatorio, atraves de uma distribuição normal, de -0.25 até 0.249..9\n",
    "    r = tf.random.uniform(shape=(), minval=0, maxval=1)* 0.5 - 0.25\n",
    "    \n",
    "    # Aplica uma rotação no sentido contrario dos relogios em radianos\n",
    "    # ou seja, aplica uma rotação de entre -14.324 a 14.324\n",
    "    image = tfa.image.rotate(image, r)\n",
    "    \n",
    "    # Devolde um numero de -10 até 9.(9)\n",
    "    rx = tf.random.uniform(shape=(), minval=0, maxval=1) * 20 - 10\n",
    "    # Devolde um numero de -4 até 3.(9)\n",
    "    ry = tf.random.uniform(shape=(), minval=0, maxval=1) * 8 - 4\n",
    "    # Aplica uma translação com o vetor [rx, ry]\n",
    "    image = tfa.image.translate(image, [rx, ry])\n",
    "    # Ajusta o hue, saturação e rgb\n",
    "    # o hue é escolhido aleatoriamente num intervalo [-0.3, 0,3]\n",
    "    # a saturaçao é escolhida aleatoriamente num intervalo [0.9, 1,1]\n",
    "    # scale_value é escolhido aleatoriamente num intervalo [0.9, 1,3]\n",
    "    image = tfa.image.random_hsv_in_yiq(image, 0.3, 0.9, 1.1, 0.9, 1.3)\n",
    "    image = tf.image.resize(image, (32,32))\n",
    "    \n",
    "    # Aumenta a brightness com um valor aleatorio de [-0.2, 0.8[\n",
    "    # Todos os ... que sejam abaixo de 0.1 passam a ser 0.1\n",
    "    image = tf.clip_by_value(tf.image.adjust_brightness(image, tf.random.uniform(shape=(), minval=0, maxval=1)-0.2),0,1)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing datasets\n",
    "\n",
    "The datasets are prepared for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(buffer_size = dataset_length)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "train_dataset = train_dataset.map(process_image)\n",
    "train_dataset = train_dataset.batch(batch_size=BATCH_SIZE)\n",
    "train_dataset = train_dataset.repeat()\n",
    "\n",
    "val_dataset = val_dataset.cache()\n",
    "val_dataset = val_dataset.shuffle(buffer_size = dataset_length)\n",
    "val_dataset = val_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "val_dataset = val_dataset.map(process_image)\n",
    "val_dataset = val_dataset.batch(batch_size=BATCH_SIZE)\n",
    "val_dataset = val_dataset.repeat()\n",
    "\n",
    "test_dataset = test_dataset.batch(batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show a batch of training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def show_batch(image_batch, label_batch):\n",
    "  columns = 6\n",
    "  rows = BATCH_SIZE / columns + 1  \n",
    "  plt.figure(figsize=(10, 2 * rows))\n",
    "  for n in range(BATCH_SIZE):\n",
    "      ax = plt.subplot(rows, columns, n+1)\n",
    "      plt.imshow((image_batch[n]))\n",
    "      plt.title(classNames[label_batch[n]==1][0])\n",
    "      plt.axis('off')\n",
    "        \n",
    "        \n",
    "image_batch, label_batch = next(iter(train_dataset))        \n",
    "show_batch(image_batch, label_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Layers\n",
    "One good strategy is to add layers to the model util it starts to overfit.\n",
    "\n",
    "#### Padding\n",
    "Bigger padding implies that the neural network must learn that the padding isn't relevant for classification. So we used padding with size one because it's the smallest padding and this way some pixels from the image are not ignored.\n",
    "\n",
    "#### Conv2D\n",
    "For the Conv2D layers, the number of filters starts at 32 and goes up, doubling it every layer. This serves to catch fewer filters at begging and more filters at the end. The Neural Network finds some features that are common by some signs at the beginning and more features at the end making the neural network recognize each sign individually.\n",
    "\n",
    "#### Dropouts\n",
    "Dropouts are used to maintain randomness. It helps to overcome overfit.\n",
    "\n",
    "#### Denses\n",
    "There are two dense layers, one with 256 nodes and one with 43 nodes. The first one is used in association that can exist among any feature to any other feature in a data point. The second gives the class name of the image.\n",
    "\n",
    "#### Model\n",
    "The model consists of 4 major groups. The first three groups are equal and consist of the following sequence:\n",
    "1. Conv2D\n",
    "2. BatchNormalization\n",
    "3. Dropout\n",
    "4. Conv2D\n",
    "5. BatchNormalization\n",
    "6. MaxPooling2D\n",
    "7. Dropout\n",
    "\n",
    "The fourth group consist of two dense layers with a BatchNormalization and a Dropout between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, BatchNormalization, ZeroPadding2D\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def cnn55D3L2FC(classCount, imgSize, channels):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(ZeroPadding2D(padding=(1,1),input_shape=(imgSize, imgSize, channels)))\n",
    "    \n",
    "    model.add(Conv2D(32, (5, 5), padding='same',activation='relu'))                        \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(64, (5, 5), padding='same',activation='relu'))                        \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(128, (5, 5), padding='same',activation='relu'))                        \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(256, (5, 5), padding='same',activation='relu'))                        \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv2D(512, (5, 5), padding='same',activation='relu'))                        \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(1024, (5, 5), padding='same',activation='relu'))                        \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    \n",
    "    #Fully conected layer\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    #Three more layers\n",
    "    model.add(Dense(256, activation = \"relu\")) #Fully connected layer\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(classCount, activation='softmax'))\n",
    "   \n",
    "\n",
    "    #opt = Adam(lr=0.001)\n",
    "    model.compile(optimizer = \"adam\", loss='categorical_crossentropy', metrics=[ metrics.categorical_accuracy])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = cnn55D3L2FC(43, 64, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw a diagram of the network\n",
    "\n",
    "This requires installing some packages, namely graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, 'multi_input_and_output_model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display a table with model information\n",
    "\n",
    "When building a model kee an eye on the number of trainable parameters. Try to keep it below 10 million"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network\n",
    "\n",
    "#### **Number of epochs**\n",
    "Similar to the number of layers, a good strategy is to increase the number of epochs until it starts to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.callbacks.callbacks import EarlyStopping\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "            monitor='val_categorical_accuracy',\n",
    "            min_delta=0,\n",
    "            patience=2, #Este patience diz que se não melhorar em 1 epoch fodeu basicamente\n",
    "            verbose=3,\n",
    "            mode='max'\n",
    "        )\n",
    "    ]\n",
    "\n",
    "history = model.fit(train_dataset, steps_per_epoch = dataset_length/BATCH_SIZE, epochs=20, validation_data = val_dataset)\n",
    "\n",
    "#history = model.fit(dataset, steps_per_epoch = 0.8*dataset_length/BATCH_SIZE,\n",
    "#          epochs=100, validation_data = val_dataset, validation_steps= 0.2*dataset_length/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.plot(history.history['val_categorical_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model on the test set\n",
    "\n",
    "This is the accuracy number that really matters. The current accuracy is 98.2%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
