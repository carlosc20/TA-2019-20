{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "\n",
    "Some references:\n",
    "\n",
    "- Transfer Learning with TF 2.0 (https://towardsdatascience.com/transfer-learning-with-tf-2-0-ff960901046d)\n",
    "- How to Classify Photos of Dogs and Cats (with 97% accuracy) (https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-to-classify-photos-of-dogs-and-cats/)\n",
    "- Models pre-trained on ImageNet (https://keras.io/applications/)\n",
    "\n",
    "Dataset: Dogs vs. Cats dataset - https://www.kaggle.com/c/dogs-vs-cats/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, metrics\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NCLASSES = 2\n",
    "HEIGHT = 224\n",
    "WIDTH = 224\n",
    "NUM_CHANNELS = 3\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "DATA_SET_COUNT = 250\n",
    "TEST_SET_COUNT = 2000\n",
    "TRAIN_SET_COUNT = (DATA_SET_COUNT) * 0.8\n",
    "VAL_SET_COUNT = (DATA_SET_COUNT) * 0.2\n",
    "print(TRAIN_SET_COUNT, VAL_SET_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a model from keras database (https://keras.io/applications/) with weights trained on ImageNet\n",
    "\n",
    "Note that the *image_top* parameter is set to *true*. This implies that we are only keeping the convolutional layers of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_model = tf.keras.applications.vgg19.VGG19(input_shape=(HEIGHT, WIDTH, NUM_CHANNELS), include_top=False, weights='imagenet')\n",
    "base_model = tf.keras.applications.inception_resnet_v2.InceptionResNetV2(include_top=False,weights=None, input_shape=(HEIGHT, WIDTH, NUM_CHANNELS))\n",
    "\n",
    "print(base_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freeze the preloaded weights, these won't be used for training\n",
    "comment the line below to train the full network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add new layers to the top of the network\n",
    "\n",
    "Global Average Pooling (GAP) performs an average for each of the feature maps. Assuming the last conv layer is 3x3x256, we have 256 feature maps. Global average pooling is a layer with 256 values, each being the average of a feature map.\n",
    "\n",
    "Original paper for Global Average Pooling: Network In Network, Min Lin1, Qiang Chen, Shuicheng Yan (https://arxiv.org/pdf/1312.4400.pdf). Note the GAP is not the main theme of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base_model.output\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(4096, activation='relu')(x)\n",
    "x = layers.Dense(2, activation='softmax')(x)\n",
    "\n",
    "model_3 = models.Model(inputs=base_model.input, outputs=x)\n",
    "print(model_3.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auxiliary function for loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(file_path):\n",
    "  # convert the path to a list of path components\n",
    "  parts = tf.strings.split(file_path, os.path.sep)\n",
    "  # The second to last is the class-directory\n",
    "  return parts[-2] == classNames\n",
    "\n",
    "def decode_img(img):\n",
    "  # convert the compressed string to a 3D uint8 tensor\n",
    "  img = tf.image.decode_png(img, channels=3)\n",
    "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "  # resize the image to the desired size.\n",
    "  return tf.image.resize(img, [WIDTH, HEIGHT])\n",
    "\n",
    "def get_bytes_and_label(file_path):\n",
    "  label = get_label(file_path)\n",
    "  # load the raw data from the file as a string\n",
    "  img = tf.io.read_file(file_path)\n",
    "  img = decode_img(img)\n",
    "  return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset, replace the path with your own location of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('D:/deepLearning/dogsVScats/split2500/train/')\n",
    "  \n",
    "classNames = np.array(os.listdir(data_dir))\n",
    "classNames\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "listset = tf.data.Dataset.list_files(\"D:/deepLearning/dogsVScats/split250/train/*/*.jpg\")\n",
    "dataset = listset.map(get_bytes_and_label, num_parallel_calls = AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "#dataset = dataset.batch(batch_size=BATCH_SIZE)\n",
    "dataset = dataset.repeat()\n",
    "\n",
    "train_size = int(TRAIN_SET_COUNT)\n",
    "val_size = int(VAL_SET_COUNT)\n",
    "train_dataset = dataset.take(train_size)\n",
    "train_dataset = train_dataset.shuffle(buffer_size = train_size)\n",
    "train_dataset = train_dataset.batch(batch_size=BATCH_SIZE)\n",
    "train_dataset = train_dataset.repeat();\n",
    "val_dataset = dataset.skip(train_size)\n",
    "val_dataset = val_dataset.batch(batch_size=BATCH_SIZE)\n",
    "val_dataset = val_dataset.repeat();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = tf.data.Dataset.list_files(\"D:/deepLearning/dogsVScats/split/test/*/*.jpg\")\n",
    "testset = testset.map(get_bytes_and_label, num_parallel_calls = AUTOTUNE)\n",
    "testset = testset.batch(batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a batch of images from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(image_batch, label_batch):\n",
    "  columns = 6\n",
    "  rows = BATCH_SIZE / columns + 1  \n",
    "  plt.figure(figsize=(10, 2 * rows))\n",
    "  for n in range(BATCH_SIZE):\n",
    "      ax = plt.subplot(rows, columns, n+1)\n",
    "      plt.imshow((image_batch[n]))\n",
    "      plt.title(classNames[label_batch[n]==1][0])\n",
    "      plt.axis('off')\n",
    "        \n",
    "        \n",
    "image_batch, label_batch = next(iter(train_dataset))  \n",
    "show_batch(image_batch, label_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model for 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[ metrics.categorical_accuracy])\n",
    "history = model_3.fit(train_dataset, epochs=20, steps_per_epoch=train_size/BATCH_SIZE, validation_data = val_dataset, validation_steps= val_size/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show how the learning progressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.plot(history.history['val_categorical_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model on the testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.evaluate(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
